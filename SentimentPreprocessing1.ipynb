{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up HDFS and Google credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://sp-master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://10.164.0.2:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Test Etienne JOB</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://10.164.0.2:7077 appName=Test Etienne JOB>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "LOCAL_IP = \"10.164.0.2\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Test Etienne JOB\") \\\n",
    "    .master(\"spark://10.164.0.2:7077\") \\\n",
    "    .config(\"spark.executor.cores\", 2) \\\n",
    "    .config(\"spark.cores.max\", 14) \\\n",
    "    .config(\"spark.python.worker.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.memory\", \"5g\") \\\n",
    "    .config(\"spark.executorEnv.SPARK_LOCAL_IP\", LOCAL_IP) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"./imdb-e9e7ce7a779d.json\"\n",
    "os.environ[\"HDFSCLI_CONFIG\"]=\"./.hdfscli.cfg\"\n",
    "os.environ[\"HADOOP_CONF_DIR\"]=\"/opt/hadoop-3.1.0/etc/hadoop\"\n",
    "sc.environment[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/MovieScope-1bf4856cc738.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List filenames of reviews from HDFS and parallelize in preparation from processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the Google Cloud client library\n",
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "from functools import reduce\n",
    "\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "from pyspark.sql import functions\n",
    "import re\n",
    "import time\n",
    "\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "orientation = \"pos\"\n",
    "collection=\"reviews\"\n",
    "urlsCollection=\"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|   ID|    ENTITY_SENTIMENT|\n",
      "+-----+--------------------+\n",
      "|10037|[[titanic, -1.800...|\n",
      "|10038|[[rose, 0.0], [ja...|\n",
      "|10039|[[titanic, 0.0], ...|\n",
      "|10040|[[titanic, 1.88],...|\n",
      "| 1004|[[masterpiece, 0....|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "pos\n"
     ]
    }
   ],
   "source": [
    "# Make sure we don't trigger Google Cloud API again\n",
    "entity_documents_info = spark.read.parquet(\"hdfs://spark-master:8020/user/lmrd/\"+collection+\"/\"+orientation+\"_doc_info2.pq\")\n",
    "entity_documents_info.show(5)\n",
    "print(orientation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load genre information from file (previously collected using IMDB API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+---+\n",
      "|FILM_ID|              GENRE| ID|\n",
      "+-------+-------------------+---+\n",
      "| 453418|[Animation, Comedy]|  1|\n",
      "| 453418|[Animation, Comedy]|  2|\n",
      "| 453418|[Animation, Comedy]|  3|\n",
      "|  64354|           [Comedy]|  4|\n",
      "|  64354|           [Comedy]|  5|\n",
      "+-------+-------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import base64\n",
    "from functools import reduce\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window as W\n",
    "\n",
    "def decodeGenre(x):\n",
    "    try: \n",
    "        g = pickle.loads(base64.b64decode(x[2:-1]), encoding=\"bytes\") \n",
    "        if (len(g)==0):\n",
    "            return [\"NA\"]\n",
    "        else:\n",
    "            return g\n",
    "    except:\n",
    "        return [\"NA\"]    \n",
    "        \n",
    "        \n",
    "genres = pd.read_csv(\"Data/genres_\"+urlsCollection+\"_urls_\"+orientation+\".csv\", sep=\"\\t\", index_col=0, usecols=[1, 2, 3])\n",
    "#print(genres.head())\n",
    "genres = genres.fillna(value=\"b''\")\n",
    "genres[\"GENRE\"] = genres[\"GENRE\"].apply(decodeGenre) \n",
    "\n",
    "# Get list of unique genre values\n",
    "#unique_genres = set(reduce(lambda x, y: x+y, genres[\"GENRE\"].values))\n",
    "#print(unique_genres)\n",
    "\n",
    "#print(genres.head())\n",
    "#print(genres[[\"ID\", \"GENRE\"]])\n",
    "#z = zip(genres[\"ID\"], genres[\"GENRE\"])\n",
    "\n",
    "\n",
    "#genres_rdd = sc.parallelize([(int(k)-1, v[0], v[1]) for (k, v) in genres.iteritems()])\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"FILM_ID\", IntegerType(), True),\n",
    "    StructField(\"GENRE\", ArrayType(StringType(), containsNull=True), True)])\n",
    "\n",
    "genres_df = spark.createDataFrame(genres, schema)\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# This will return a new DF with all the columns + id\n",
    "genres_df = genres_df.withColumn(\"ID_TEMP\", monotonically_increasing_id())#.limit(10)\n",
    "\n",
    "genres_df = genres_df.withColumn(\"ID\",F.row_number().over(W.orderBy(\"ID_TEMP\"))).select([\"FILM_ID\", \"GENRE\", \"ID\"])#.limit(10)\n",
    "\n",
    "#df1.withColumn(\"idx\", F.row_number())\n",
    "genres_df.show(5)\n",
    "#genres_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------+-------------------+---+\n",
      "| ID|    ENTITY_SENTIMENT|FILM_ID|              GENRE| ID|\n",
      "+---+--------------------+-------+-------------------+---+\n",
      "|  1|[[bromwell high, ...| 453418|[Animation, Comedy]|  1|\n",
      "|  2|[[format, 0.0], [...| 453418|[Animation, Comedy]|  2|\n",
      "|  3|[[bromwell high, ...| 453418|[Animation, Comedy]|  3|\n",
      "|  4|[[world, 0.0], [s...|  64354|           [Comedy]|  4|\n",
      "|  5|[[futz, 0.0], [pi...|  64354|           [Comedy]|  5|\n",
      "+---+--------------------+-------+-------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "entity_documents_info = entity_documents_info.alias(\"df1\").join(genres_df.alias(\"df2\"), entity_documents_info.ID == genres_df.ID)#.select([\"df1.*\", \"df2.FILM_ID\", \"df2.GENRE\"])\n",
    "\n",
    "entity_documents_info.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group documents by genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[genre: string, entity: string, sentiment: double]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def separateGenres(rec):\n",
    "    print(len(rec))\n",
    "    return [[genre, rec[0]] for genre in rec[1][1]]\n",
    "\n",
    "def separateGenres2(rec):\n",
    "    return [[genre, e, s] for (e, s) in rec[0] for genre in rec[1][1]]\n",
    "\n",
    "def separateGenres3(rec):\n",
    "    print(rec)\n",
    "    return [[genre, e, s] for (e, s) in rec.ENTITY_SENTIMENT for genre in rec.GENRE]\n",
    "    \n",
    "#grouped_entities = entity_documents_info.flatMap(separateGenres).reduceByKey(collectEntities)\n",
    "grouped_entities = entity_documents_info.rdd.flatMap(separateGenres3)\n",
    "grouped_entities.repartition(5)\n",
    "grouped_entities_df = spark.createDataFrame(data=grouped_entities, schema=[\"genre\", \"entity\", \"sentiment\"])\n",
    "#grouped_entities_df.show()\n",
    "grouped_entities_df.cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Animation', 'bromwell high', 0.0],\n",
       " ['Comedy', 'bromwell high', 0.0],\n",
       " ['Animation', 'teacher', 0.0],\n",
       " ['Comedy', 'teacher', 0.0],\n",
       " ['Animation', 'program', 0.0],\n",
       " ['Comedy', 'program', 0.0],\n",
       " ['Animation', 'school life', 0.0],\n",
       " ['Comedy', 'school life', 0.0],\n",
       " ['Animation', 'student', 0.0],\n",
       " ['Comedy', 'student', 0.0],\n",
       " ['Animation', 'episode', 0.0],\n",
       " ['Comedy', 'episode', 0.0],\n",
       " ['Animation', 'teaching profession', 0.0],\n",
       " ['Comedy', 'teaching profession', 0.0],\n",
       " ['Animation', 'teacher', -0.809999942779541],\n",
       " ['Comedy', 'teacher', -0.809999942779541],\n",
       " ['Animation', 'student', -0.04000000283122063],\n",
       " ['Comedy', 'student', -0.04000000283122063],\n",
       " ['Animation', 'student', -0.010000000707805157],\n",
       " ['Comedy', 'student', -0.010000000707805157],\n",
       " ['Animation', 'satire', 0.0],\n",
       " ['Comedy', 'satire', 0.0],\n",
       " ['Animation', 'school', -0.04000000283122063],\n",
       " ['Comedy', 'school', -0.04000000283122063],\n",
       " ['Animation', 'situation', 0.0],\n",
       " ['Comedy', 'situation', 0.0],\n",
       " ['Animation', 'pettines', -0.010000000707805157],\n",
       " ['Comedy', 'pettines', -0.010000000707805157],\n",
       " ['Animation', 'pomp', 0.0],\n",
       " ['Comedy', 'pomp', 0.0],\n",
       " ['Animation', 'all', 0.0],\n",
       " ['Comedy', 'all', 0.0],\n",
       " ['Animation', 'reality', 0.0],\n",
       " ['Comedy', 'reality', 0.0],\n",
       " ['Animation', 'scramble', -0.04000000283122063],\n",
       " ['Comedy', 'scramble', -0.04000000283122063],\n",
       " ['Animation', 'teacher', 0.0],\n",
       " ['Comedy', 'teacher', 0.0],\n",
       " ['Animation', 'student', 0.0],\n",
       " ['Comedy', 'student', 0.0],\n",
       " ['Animation', 'one', -0.010000000707805157],\n",
       " ['Comedy', 'one', -0.010000000707805157],\n",
       " ['Animation', 'adult', 0.0],\n",
       " ['Comedy', 'adult', 0.0],\n",
       " ['Animation', 'age', 0.0],\n",
       " ['Comedy', 'age', 0.0],\n",
       " ['Animation', 'pity', 0.0],\n",
       " ['Comedy', 'pity', 0.0],\n",
       " ['Animation', 'school', -0.09000000357627869],\n",
       " ['Comedy', 'school', -0.09000000357627869]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_entities.take(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouped_entities_df.show(5)\n",
    "grouped_entities_df.write.parquet(\"hdfs://spark-master:8020/user/lmrd/\"+collection+\"/\"+urlsCollection+\"_\"+orientation+\"_grouped_entities2.pq\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8d3513b7698b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
