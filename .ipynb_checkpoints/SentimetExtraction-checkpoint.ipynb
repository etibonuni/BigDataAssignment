{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up HDFS and Google credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://sp-master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://sp-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://sp-master:7077 appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"./imdb-e9e7ce7a779d.json\"\n",
    "os.environ[\"HDFSCLI_CONFIG\"]=\"./.hdfscli.cfg\"\n",
    "os.environ[\"HADOOP_CONF_DIR\"]=\"/opt/hadoop-3.1.0/etc/hadoop\"\n",
    "sc.environment[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/MovieScope-1bf4856cc738.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List filenames of reviews from HDFS and parallelize in preparation from processing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from hdfs import Config\n",
    "import pandas as pd\n",
    "\n",
    "#9870\n",
    "#from hdfs3 import HDFileSystem\n",
    "#hdfs = HDFileSystem(u\"localhost\", 9000)\n",
    "\n",
    "\n",
    "hdfs_prefix=\"hdfs://localhost:9000/\"\n",
    "reviews_path=\"/user/lmrd/reviews\"\n",
    "\n",
    "    \n",
    "def getReviewFilenames(path):\n",
    "    client = Config().get_client('dev')\n",
    "    files = client.list(path)\n",
    "    \n",
    "    files_df = pd.DataFrame(data={\"fileNames\":files})\n",
    "    \n",
    "    files_df[\"index\"] = files_df[\"fileNames\"].str.extract('(.*)_.*\\..*', expand = True).apply(pd.to_numeric)\n",
    "\n",
    "    files_df = files_df.sort_values(by=\"index\")\n",
    "   # print(files_df)\n",
    "    \n",
    "    return [hdfs_prefix+path+\"/\"+f for f in files_df[\"fileNames\"]]\n",
    "\n",
    "pos_files_rdd = sc.parallelize(getReviewFilenames(reviews_path+\"/pos\"))\n",
    "#pos_files_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelise the reviews and use Google NLP API to extract entities and related sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the Google Cloud client library\n",
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "from functools import reduce\n",
    "\n",
    "def collectEntities(x, y):\n",
    "    # The first reduce call doesn't pass a list for x, so we need to check for that.\n",
    "    if not isinstance(x, list):\n",
    "        x=[x]\n",
    "        \n",
    "\n",
    "    xd = dict(x)\n",
    "    #print(xd)\n",
    "    \n",
    "    if not isinstance(y, list):\n",
    "        y = [y]\n",
    "        \n",
    "    for ye in y:\n",
    "        if ye[0] in xd:\n",
    "            try:\n",
    "                xd[ye[0]] = (xd[ye[0]]+ye[1])/2\n",
    "            except:\n",
    "                Null\n",
    "        else:\n",
    "            xd[ye[0]] = ye[1]\n",
    "    \n",
    "    return [o for o in xd.items()]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import re\n",
    "\n",
    "def extractEntitiesSetiment(fileObj):\n",
    "    # Instantiates a client\n",
    "    client = language.LanguageServiceClient()\n",
    "    \n",
    "    review_contents = fileObj\n",
    "        \n",
    "    #print(review_contents)\n",
    "    \n",
    "    document = types.Document(content = review_contents, \n",
    "                             type=enums.Document.Type.PLAIN_TEXT)\n",
    "    \n",
    "    entities = client.analyze_entity_sentiment(document=document, encoding_type=\"UTF8\")\n",
    "    \n",
    "    # Make sure we have no duplicate entities. If we do, average their sentiment.\n",
    "    justLetters = re.compile(\"[^a-z ]\")\n",
    "    response = [o for o in zip([justLetters.sub(\"\", entity.name.lower()) for entity in entities.entities], [entity.sentiment.score * entity.sentiment.magnitude for entity in entities.entities])]\n",
    "    response = sorted(response, key=lambda x: x[0])\n",
    "    response = reduce(collectEntities, response)\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "\n",
    "file_objs = map(lambda f: ' '.join(list(sc.hadoopFile(f, \"org.apache.hadoop.mapred.TextInputFormat\", \n",
    "                                              \"org.apache.hadoop.io.Text\", \n",
    "                                              \"org.apache.hadoop.io.Text\").values().collect())), pos_files_rdd.collect())\n",
    "\n",
    "#file_objs = sc.parallelize(file_objs)\n",
    "#entity_documents_info = file_objs.map(extractEntitiesSetiment)\n",
    "\n",
    "\n",
    "#entity_documents_info.cache()\n",
    "#print(entity_documents_info.take(1))\n",
    "#print(entity_documents_info.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "orientation = \"neg\"\n",
    "collection=\"reviews\"\n",
    "urlsCollection=\"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = sc.wholeTextFiles(\"hdfs://sp-master:8020/user/lmrd/\"+collection+\"/\"+orientation)\n",
    "tf = tf.repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hdfs://sp-master:8020/user/lmrd/test_reviews/neg/10036_1.txt',\n",
       "  \"Are you kidding me? This is quite possibly the worst, amateur movie I've ever seen. The casting was horrible, the acting was worse than horrible and I'm sorry, the guy at the picnic speed loading his plate full of food was somewhere near pointless and the demonic turd and chamber pot chasing Drew around was nothing more than comical. When I herd about the Bell Witch, I wanted to believe. I read some literature on it and thought it sounded like it was possible a plausible story. But this movie just destroyed that. Ric White (Director, Writer, Lead Actor, etc) takes himself a bit too seriously and I think he gives himself a little more credit than he deserves....Do yourself a favor....skip this one.\"),\n",
       " ('hdfs://sp-master:8020/user/lmrd/test_reviews/neg/10037_1.txt',\n",
       "  'In addition to the fact that this is just an abysmally made film (imagine giving a camcorder to the average high school drama club) the people who think that there is anything \"real\" about this legend need to grow up. This is the 21st century. Guess what: ghosts don\\'t exist. Most people learn that from their mother when they\\'re about 5 years old. You guys seriously need to grow up.<br /><br />The fact that a fraud was perpetrated nearly 2 centuries ago does not make it any less a fraud. The fact that a large number of inbred hillbillies from Tennessee believe it doesn\\'t do it either. Go to college. Or at least finish high school.'),\n",
       " ('hdfs://sp-master:8020/user/lmrd/test_reviews/neg/10038_4.txt',\n",
       "  'I wanted to like this movie. I really, really did. I was so excited when I saw the preview, which scared the hell out of me. But when I saw the actual film, I was disappointed. The acting is stilted, and the attempts at comedy are woefully out of place and forced. And I\\'m sorry, but a boy being chased by a turd in a bedpan is not funny or scary, it\\'s just stupid. I grew up on the Bell Witch legend, so I know quite a bit about it. A lot of facts in the movie are right on target, but this film should have been much better. The entire birthday party scene, for example, lasts about fifteen minutes, adds nothing to the plot or the story, and should have been left on the cutting room floor. A more heavy-handed editor might have been able to get a decent film out of this mess.<br /><br />Please understand, I\\'m not in any way, shape or form involved with the other Bell Witch movie, and I\\'m not trying to \"attack\" this IMDb listing. I\\'m just telling it like it is.'),\n",
       " ('hdfs://sp-master:8020/user/lmrd/test_reviews/neg/10039_1.txt',\n",
       "  'I am not understanding why people are praising this movie. I didn\\'t like it at all. I watch it with several people. None of them cared for it either. First of all. It is just plain that another low budget studio is trying to cash in on a big name story. The actual filming looks like a live TV interview. The makeup is bad. When you watch the movie along with the DVD extras. You will see there is a lot of enthusiasm from the people who participated in it. There is no talent. There are facts that do appear in the book. The facts are distorted by the invention of comedy and skits added to it. I have read several books and have watched several shows on this story. What I have always caught from all the material on this is that it was a serious horror story. I really wish someone could really do a good film on this one. It has always fascinated me. The bad acting really ruined the story. The little boys situation really hammed it up even more. When you watch this movie. The little boy and his problem is the thing you and your friends will remember and laugh about. It didn\\'t make any sense why his brothers were laughing at what had happened to him. It was like the witch was supposed to be so threatening but it was OK to throw baby brother to her. It is a whopping tale with him and his little problem. I can\\'t still get over the little girl saying \"Mom said tobacco will rot your teeth.\" Frank Fox\\'s statement and facial expression is so bad. The scene out in the yard with him getting food is pretty stupid to. The sound from parts of it seems to be from the movie psycho. Also, The girl hovering over the bed and her little \"Bladder control problem\" are from The Exorcist. This movie is lacking from the talent of creativity. We put the movie in for a couple of minutes and knew right away it was a bummer. I also noticed that their was defects in the film quality. Parts of it looked like what a person might film on a Home video camera. I noticed a lot of the people in the credits had many multiple jobs. This is probably how this movie was put together. Someone said I like this story. I will get all my friends and make a movie about with a video camera and a computer. Doesn\\'t matter if we don\\'t know how to act. As long as we get it on film and say it is good. We got the family together and prepared food. Then sat down and watched this failed attempt to make a movie.'),\n",
       " ('hdfs://sp-master:8020/user/lmrd/test_reviews/neg/1003_4.txt',\n",
       "  'This tale of the upper-classes getting their come-uppance and wallowing in their high-class misery is like a contemporary Mid-Sommerish version of an old Joan Crawford movie in which she suffered in mink. Here, people behave in a frightfully civilized manner in the face of adversity. A well-heeled London solicitor, (Tom Wilkinson), discovers that not only is his wife having an affair with the local gentry but that she has also killed their housekeeper\\'s husband in a hit-and-run accident. He throws up, but otherwise his stiff-upper-lip hardly quavers.<br /><br />Written and directed by Julian Fellowes, who won an Oscar for writing \"Gosford Park\", (this is his directorial debut), from a novel by Nigel Balchin, it\\'s quite comical although I am not sure how much of the comedy is intended. It\\'s like a throw-back to British films of the forties where characters all behaved like characters in books or plays rather than like people might in real life. However, it\\'s not all bad. Wilkinson is terrific, even if you never believe in him as a person while Emily Watson, (the adulterous wife), and Rupert Everett, (the highly amoral high-class totty), are both very good at covering the cracks in the material. Tony Pierce-Roberts\\' cinematography ensures that no matter how hard it is on the ear it\\'s always good on the eye.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def checkSentimentValue(x):\n",
    "    try:\n",
    "        f = float(x)\n",
    "        \n",
    "        return f\n",
    "    \n",
    "    except:\n",
    "        print(\"Wrong sentiment value \", f)\n",
    "        return 0\n",
    "    \n",
    "def extractEntitiesSetiment2(fileObj):\n",
    "    # Instantiates a client\n",
    "    client = language.LanguageServiceClient()\n",
    "    \n",
    "    review_contents = fileObj[1]\n",
    "        \n",
    "    #print(review_contents)\n",
    "    document = types.Document(content = review_contents, \n",
    "                             type=enums.Document.Type.PLAIN_TEXT, language=\"en-US\")\n",
    "    \n",
    "    tries=1\n",
    "    \n",
    "    while tries < 5:\n",
    "        try:\n",
    "            entities = client.analyze_entity_sentiment(document=document, encoding_type=\"UTF8\")\n",
    "            break\n",
    "        except:\n",
    "            f = open(\"/home/etienne/sparklog.txt\", mode=\"a\")\n",
    "            f.write(\"\"+str(fileObj[0])+\"\\n\")\n",
    "            f.write(\"\"+str(entities)+\"\\n\")\n",
    "            f.close()\n",
    "            time.sleep(1)\n",
    "            \n",
    "            tries +=1\n",
    "    \n",
    "    \n",
    "    # Make sure we have no duplicate entities. If we do, average their sentiment.\n",
    "    justLetters = re.compile(\"[^a-z ]\")\n",
    "    response = [o for o in zip([justLetters.sub(\"\", entity.name.lower()) for entity in entities.entities], \n",
    "                               [checkSentimentValue(entity.sentiment.score) * checkSentimentValue(entity.sentiment.magnitude) \n",
    "                                    for entity in entities.entities])]\n",
    "    \n",
    "    response = sorted(response, key=lambda x: x[0])\n",
    "    if (len(response)>1):\n",
    "        response = reduce(collectEntities, response)\n",
    "    \n",
    "        \n",
    "    #print(fileObj[0], response)\n",
    "    try:\n",
    "        fid = int(fileObj[0])\n",
    "    except:\n",
    "        fid=0\n",
    "    \n",
    "    return (fid, response)\n",
    "\n",
    "def extractOrdering(rec):\n",
    "    filenameRegexp = \".*/([0-9]*)_.*\\.txt$\"\n",
    "    r = re.search(filenameRegexp, rec[0])\n",
    "\n",
    "    return (int(r.groups()[0])+1, rec[1])\n",
    "    #hdfs://localhost:9000/user/lmrd/reviews/pos/3467_7.txt\n",
    "\n",
    "\n",
    "#sc.broadcast(filenameRegexp)\n",
    "filesRdd = tf.map(extractOrdering)\n",
    "filesRdd = filesRdd.repartition(5)\n",
    "\n",
    "schema1 = StructType([\n",
    "    StructField(\"ID\", IntegerType(), False),\n",
    "    StructField(\"ENTITY_SENTIMENT\", ArrayType(\n",
    "            StructType([StructField(\"ENTITY\", StringType(), False), \n",
    "                        StructField(\"SENTIMENT\", FloatType(), False)])), nullable=True)])\n",
    "\n",
    "\n",
    "entity_documents_info = filesRdd.map(extractEntitiesSetiment2)\n",
    "\n",
    "entity_documents_info.cache()\n",
    "#entity_documents_info.saveAsTextFile(\"hdfs://sp-master:8020/user/lmrd/reviews/temp_pos3.txt\")\n",
    "\n",
    "\n",
    "entity_documents_info = spark.createDataFrame(filesRdd.map(extractEntitiesSetiment2), schema1)#schema=[\"ID\", \"ENTITIY_SENTIMENT\"])\n",
    "#entity_documents_info = entity_documents_info.rdd.repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_documents_info.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#entity_documents_info = spark.createDataFrame(filesRdd.map(extractEntitiesSetiment2), schema=[\"ID\", \"ENTITIY_SENTIMENT\"])\n",
    "\n",
    "entity_documents_info.write.parquet(\"hdfs://spark-master:8020/user/lmrd/\"+collection+\"/\"+orientation+\"_doc_info.pq\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|   ID|    ENTITY_SENTIMENT|\n",
      "+-----+--------------------+\n",
      "|10127|[[adventure, 0.48...|\n",
      "|10128|[[back garden, -0...|\n",
      "|10129|[[acting, -0.09],...|\n",
      "|10130|[[acting, -0.9799...|\n",
      "| 1013|[[actors, 0.0], [...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make sure we don't trigger Google Cloud API again\n",
    "entity_documents_info = spark.read.parquet(\"hdfs://spark-master:8020/user/lmrd/\"+collection+\"/\"+orientation+\"_doc_info.pq\")\n",
    "entity_documents_info.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load genre information from file (previously collected using IMDB API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---+\n",
      "|FILM_ID|               GENRE| ID|\n",
      "+-------+--------------------+---+\n",
      "|  64354|            [Comedy]|  1|\n",
      "| 100680|    [Drama, Romance]|  2|\n",
      "| 100680|    [Drama, Romance]|  3|\n",
      "| 100680|    [Drama, Romance]|  4|\n",
      "|  47200|[Horror, Mystery,...|  5|\n",
      "+-------+--------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import base64\n",
    "from functools import reduce\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window as W\n",
    "\n",
    "def decodeGenre(x):\n",
    "    try: \n",
    "        g = pickle.loads(base64.b64decode(x[2:-1]), encoding=\"bytes\") \n",
    "        if (len(g)==0):\n",
    "            return [\"NA\"]\n",
    "        else:\n",
    "            return g\n",
    "    except:\n",
    "        return [\"NA\"]    \n",
    "        \n",
    "        \n",
    "genres = pd.read_csv(\"Data/genres_\"+urlsCollection+\"_urls_\"+orientation+\".csv\", sep=\"\\t\", index_col=0, usecols=[1, 2, 3])\n",
    "#print(genres.head())\n",
    "genres = genres.fillna(value=\"b''\")\n",
    "genres[\"GENRE\"] = genres[\"GENRE\"].apply(decodeGenre) \n",
    "\n",
    "# Get list of unique genre values\n",
    "#unique_genres = set(reduce(lambda x, y: x+y, genres[\"GENRE\"].values))\n",
    "#print(unique_genres)\n",
    "\n",
    "#print(genres.head())\n",
    "#print(genres[[\"ID\", \"GENRE\"]])\n",
    "#z = zip(genres[\"ID\"], genres[\"GENRE\"])\n",
    "\n",
    "\n",
    "#genres_rdd = sc.parallelize([(int(k)-1, v[0], v[1]) for (k, v) in genres.iteritems()])\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"FILM_ID\", IntegerType(), True),\n",
    "    StructField(\"GENRE\", ArrayType(StringType(), containsNull=True), True)])\n",
    "\n",
    "genres_df = spark.createDataFrame(genres, schema)\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# This will return a new DF with all the columns + id\n",
    "genres_df = genres_df.withColumn(\"ID_TEMP\", monotonically_increasing_id())#.limit(10)\n",
    "\n",
    "genres_df = genres_df.withColumn(\"ID\",F.row_number().over(W.orderBy(\"ID_TEMP\"))).select([\"FILM_ID\", \"GENRE\", \"ID\"])#.limit(10)\n",
    "\n",
    "#df1.withColumn(\"idx\", F.row_number())\n",
    "genres_df.show(5)\n",
    "#genres_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------+--------------------+---+\n",
      "| ID|    ENTITY_SENTIMENT|FILM_ID|               GENRE| ID|\n",
      "+---+--------------------+-------+--------------------+---+\n",
      "|  1|[[chantings, -0.0...|  64354|            [Comedy]|  1|\n",
      "|  2|[[book, 0.0], [ca...| 100680|    [Drama, Romance]|  2|\n",
      "|  3|[[acting, 0.48999...| 100680|    [Drama, Romance]|  3|\n",
      "|  4|[[adaptation, -0....| 100680|    [Drama, Romance]|  4|\n",
      "|  5|[[another, 0.0100...|  47200|[Horror, Mystery,...|  5|\n",
      "+---+--------------------+-------+--------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "entity_documents_info = entity_documents_info.alias(\"df1\").join(genres_df.alias(\"df2\"), entity_documents_info.ID == genres_df.ID)#.select([\"df1.*\", \"df2.FILM_ID\", \"df2.GENRE\"])\n",
    "\n",
    "entity_documents_info.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zip the document-entity-sentiment rdd with the genre rdd.\n",
    "There should be exactly the same number of reviews as records in the genres rdd."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "entity_documents_info = entity_documents_info.zip(genres_rdd)\n",
    "\n",
    "#entity_documents_info.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group documents by genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[genre: string, entity: string, sentiment: double]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def separateGenres(rec):\n",
    "    print(len(rec))\n",
    "    return [[genre, rec[0]] for genre in rec[1][1]]\n",
    "\n",
    "def separateGenres2(rec):\n",
    "    return [[genre, e, s] for (e, s) in rec[0] for genre in rec[1][1]]\n",
    "\n",
    "def separateGenres3(rec):\n",
    "    print(rec)\n",
    "    return [[genre, e, s] for (e, s) in rec.ENTITY_SENTIMENT for genre in rec.GENRE]\n",
    "    \n",
    "#grouped_entities = entity_documents_info.flatMap(separateGenres).reduceByKey(collectEntities)\n",
    "grouped_entities = entity_documents_info.rdd.flatMap(separateGenres3)\n",
    "grouped_entities.repartition(5)\n",
    "grouped_entities_df = spark.createDataFrame(data=grouped_entities, schema=[\"genre\", \"entity\", \"sentiment\"])\n",
    "#grouped_entities_df.show()\n",
    "grouped_entities_df.cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouped_entities_df.show(5)\n",
    "grouped_entities_df.write.parquet(\"hdfs://spark-master:8020/user/lmrd/\"+urlsCollection+\"_\"+orientation+\"_grouped_entities.pq\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import collect_list\n",
    "\n",
    "grouped_entities_df = spark.read.parquet(\"hdfs://spark-master:8020/user/lmrd/\"+urlsCollection+\"_\"+orientation+\"_grouped_entities.pq\")\n",
    "\n",
    "grouped_entity_words = grouped_entities_df.select([\"genre\", \"entity\"]).groupBy(\"genre\").agg(collect_list(\"entity\").alias(\"entities\"))\n",
    "grouped_sentiment = grouped_entities_df.select([\"genre\", \"sentiment\"]).groupBy(\"genre\").agg(collect_list(\"sentiment\").alias(\"sentiment\"))\n",
    "#grouped_entity_words.show()\n",
    "#grouped_sentiment.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|      genre|           sentiment|\n",
      "+-----------+--------------------+\n",
      "|      Crime|[0.0, 0.010000000...|\n",
      "|    Romance|[0.0, 0.0, 0.8099...|\n",
      "|   Thriller|[-0.0100000007078...|\n",
      "|  Adventure|[-0.0400000028312...|\n",
      "|         NA|[0.0, 0.0, -0.489...|\n",
      "|      Drama|[-0.0400000028312...|\n",
      "|        War|[0.0, 0.0, 0.8099...|\n",
      "|Documentary|[0.0, 0.0, 0.0, 0...|\n",
      "| Reality-TV|[0.0, 0.0, 0.0, 0...|\n",
      "|     Family|[0.0, 0.0, -0.040...|\n",
      "|    Fantasy|[0.01000000070780...|\n",
      "|  Game-Show|[0.0, 0.0, 0.0, 0...|\n",
      "|      Adult|[-0.25, -0.25, -0...|\n",
      "|    History|[0.0, 0.0, 0.0, 0...|\n",
      "|    Mystery|[-0.0400000028312...|\n",
      "|    Musical|[0.0, 0.0, 0.8099...|\n",
      "|  Animation|[0.0, 0.0, -0.040...|\n",
      "|      Music|[0.0, -0.04000000...|\n",
      "|  Film-Noir|[0.0, 0.0, 0.0, 0...|\n",
      "|      Short|[0.0, 0.0, -0.040...|\n",
      "+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_sentiment.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+\n",
      "|      genre|            entities|                  tf|\n",
      "+-----------+--------------------+--------------------+\n",
      "|      Crime|[all, anger, best...|(74427,[0,1,2,3,4...|\n",
      "|    Romance|[book, caricature...|(74427,[0,1,2,3,4...|\n",
      "|   Thriller|[another, cast, e...|(74427,[0,1,2,3,4...|\n",
      "|  Adventure|[action, artists,...|(74427,[0,1,2,3,4...|\n",
      "|         NA|[any, attachment,...|(74427,[0,1,2,3,4...|\n",
      "|      Drama|[book, caricature...|(74427,[0,1,2,3,4...|\n",
      "|        War|[actors, addition...|(74427,[0,1,2,3,4...|\n",
      "|Documentary|[absurd, action, ...|(74427,[0,1,2,3,4...|\n",
      "| Reality-TV|[all, ass, compan...|(74427,[2,3,4,6,7...|\n",
      "|     Family|[action, artists,...|(74427,[0,1,2,3,4...|\n",
      "|    Fantasy|[anything, barrag...|(74427,[0,1,2,3,4...|\n",
      "|  Game-Show|[all, ass, compan...|(74427,[2,3,6,7,1...|\n",
      "|      Adult|[adult movies, be...|(74427,[0,1,2,3,4...|\n",
      "|    History|[aaron sherritt, ...|(74427,[0,1,2,3,4...|\n",
      "|    Mystery|[another, cast, e...|(74427,[0,1,2,3,4...|\n",
      "|    Musical|[acting, all, aud...|(74427,[0,1,2,3,4...|\n",
      "|  Animation|[betty, bore, car...|(74427,[0,1,2,3,4...|\n",
      "|      Music|[barrel, boredom,...|(74427,[0,1,2,3,4...|\n",
      "|  Film-Noir|[actors, bogart, ...|(74427,[0,1,2,3,4...|\n",
      "|     Horror|[another, cast, e...|(74427,[0,1,2,3,4...|\n",
      "+-----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------+--------------------+--------------------+--------------------+\n",
      "|      genre|            entities|                  tf|               tfidf|\n",
      "+-----------+--------------------+--------------------+--------------------+\n",
      "|      Crime|[all, anger, best...|(74427,[0,1,2,3,4...|(74427,[0,1,2,3,4...|\n",
      "|    Romance|[book, caricature...|(74427,[0,1,2,3,4...|(74427,[0,1,2,3,4...|\n",
      "|   Thriller|[another, cast, e...|(74427,[0,1,2,3,4...|(74427,[0,1,2,3,4...|\n",
      "|  Adventure|[action, artists,...|(74427,[0,1,2,3,4...|(74427,[0,1,2,3,4...|\n",
      "|         NA|[any, attachment,...|(74427,[0,1,2,3,4...|(74427,[0,1,2,3,4...|\n",
      "|      Drama|[book, caricature...|(74427,[0,1,2,3,4...|(74427,[0,1,2,3,4...|\n",
      "|        War|[actors, addition...|(74427,[0,1,2,3,4...|(74427,[0,1,2,3,4...|\n",
      "|Documentary|[absurd, action, ...|(74427,[0,1,2,3,4...|(74427,[0,1,2,3,4...|\n",
      "| Reality-TV|[all, ass, compan...|(74427,[2,3,4,6,7...|(74427,[2,3,4,6,7...|\n",
      "|     Family|[action, artists,...|(74427,[0,1,2,3,4...|(74427,[0,1,2,3,4...|\n",
      "|    Fantasy|[anything, barrag...|(74427,[0,1,2,3,4...|(74427,[0,1,2,3,4...|\n",
      "|  Game-Show|[all, ass, compan...|(74427,[2,3,6,7,1...|(74427,[2,3,6,7,1...|\n",
      "|      Adult|[adult movies, be...|(74427,[0,1,2,3,4...|(74427,[0,1,2,3,4...|\n",
      "|    History|[aaron sherritt, ...|(74427,[0,1,2,3,4...|(74427,[0,1,2,3,4...|\n",
      "|    Mystery|[another, cast, e...|(74427,[0,1,2,3,4...|(74427,[0,1,2,3,4...|\n",
      "|    Musical|[acting, all, aud...|(74427,[0,1,2,3,4...|(74427,[0,1,2,3,4...|\n",
      "|  Animation|[betty, bore, car...|(74427,[0,1,2,3,4...|(74427,[0,1,2,3,4...|\n",
      "|      Music|[barrel, boredom,...|(74427,[0,1,2,3,4...|(74427,[0,1,2,3,4...|\n",
      "|  Film-Noir|[actors, bogart, ...|(74427,[0,1,2,3,4...|(74427,[0,1,2,3,4...|\n",
      "|     Horror|[another, cast, e...|(74427,[0,1,2,3,4...|(74427,[0,1,2,3,4...|\n",
      "+-----------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "\n",
    "# remove sentiment info for use by hashingTF/tfif\n",
    "\n",
    "# Load documents (one per line).\n",
    "\n",
    "countVec = CountVectorizer(inputCol=\"entities\", outputCol=\"tf\")\n",
    "cvmodel = countVec.fit(grouped_entity_words)\n",
    "\n",
    "tf = cvmodel.transform(grouped_entity_words)\n",
    "tf.show()\n",
    "#sc.broadcast(hashingTF)\n",
    "\n",
    "# While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:\n",
    "# First to compute the IDF vector and second to scale the term frequencies by IDF.\n",
    "tf.cache()\n",
    "idf = IDF(inputCol=\"tf\", outputCol=\"tfidf\").fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "tfidf.show()\n",
    "# spark.mllib's IDF implementation provides an option for ignoring terms\n",
    "# which occur in less than a minimum number of documents.\n",
    "# In such cases, the IDF for these terms is set to 0.\n",
    "# This feature can be used by passing the minDocFreq value to the IDF constructor.\n",
    "# idfIgnore = IDF(minDocFreq=2).fit(tf)\n",
    "# tfidfIgnore = idfIgnore.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import explode\n",
    "import numpy as np\n",
    "\n",
    "vocab = tfidf.select([\"genre\", \"tfidf\"])\n",
    "genreVocabs = dict()\n",
    "\n",
    "for genre in vocab.collect():\n",
    "    genreName = genre.genre\n",
    "    \n",
    "    t=genre.tfidf\n",
    "    genreVocabs[genreName] = t\n",
    "    \n",
    "globalVocab = list(cvmodel.vocabulary)\n",
    "    \n",
    "sc.broadcast(globalVocab)\n",
    "sc.broadcast(genreVocabs)\n",
    "\n",
    "def remapEntitiesByTfidf(row):\n",
    "    tfidfMappings = genreVocabs[row.genre]\n",
    "    tfIndex = globalVocab.index(row.entity)\n",
    "    tfidf = tfidfMappings[tfIndex]\n",
    "    \n",
    "    return Row(genre=row.genre, entity=row.entity, tfidf=float(tfidf), vocabIndex=int(tfIndex))\n",
    "    \n",
    "genreCorpora=dict()\n",
    "\n",
    "for genre in genreVocabs.keys():\n",
    "    genreEntities = tfidf.where(tfidf.genre==genre).select(\"genre\", explode(\"entities\").alias(\"entity\"))\n",
    "    \n",
    "    #genreEntities.show()\n",
    "    \n",
    "    #data = genreEntities.rdd.map(remapEntitiesByTfidf)\n",
    "\n",
    "    entitiesByTfidf = spark.createDataFrame(data=genreEntities.rdd.map(remapEntitiesByTfidf), schema=[\"entity\", \"genre\", \"tfidf\", \"vocabIndex\"])\n",
    "    #entitiesByTfidf.show()\n",
    "    entitiesByTfidf = entitiesByTfidf.join(grouped_entities_df, on=[\"genre\", \"entity\"], how=\"inner\" ).groupBy([\"genre\", \"entity\", \"tfidf\", \"vocabIndex\"]).avg(\"sentiment\").sort(\"tfidf\", ascending=False)\n",
    "    \n",
    "    genreCorpora[genre] = entitiesByTfidf\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crime\n",
      "Romance\n",
      "Thriller\n",
      "Adventure\n",
      "NA\n",
      "Drama\n",
      "War\n",
      "Documentary\n",
      "Reality-TV\n",
      "Family\n",
      "Fantasy\n",
      "Game-Show\n",
      "Adult\n",
      "History\n",
      "Mystery\n",
      "Musical\n",
      "Animation\n",
      "Music\n",
      "Film-Noir\n",
      "Horror\n",
      "Short\n",
      "Western\n",
      "Biography\n",
      "Comedy\n",
      "Action\n",
      "Sport\n",
      "Talk-Show\n",
      "Sci-Fi\n",
      "News\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "for (genreName, corpus) in genreCorpora.items():\n",
    "    print(genreName)\n",
    "    df = corpus.select(col(\"genre\"), col(\"entity\"), col(\"tfidf\"), col(\"vocabIndex\"), col(\"avg(sentiment)\").alias(\"sentiment\"))\n",
    "\n",
    "    df.write.parquet(\"hdfs://spark-master:8020/user/lmrd/\"+genreName+\"_\"+orientation+\"_tfidf.pq\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_df = spark.read.parquet(\"hdfs://spark-master:8020/user/lmrd/Crime_pos_tfidf.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+-----------------+----------+--------------------+\n",
      "|genre|         entity|            tfidf|vocabIndex|           sentiment|\n",
      "+-----+---------------+-----------------+----------+--------------------+\n",
      "|Crime|bullfight scene|1.791759469228055|     27869|-0.09000000357627869|\n",
      "|Crime|     dan kolton|1.791759469228055|     29876|                 0.0|\n",
      "|Crime|     family dog|1.791759469228055|     26937|-0.04000000283122063|\n",
      "|Crime|    go hk films|1.791759469228055|     36170|                 0.0|\n",
      "|Crime| grandpa walton|1.791759469228055|     31916|                 0.0|\n",
      "+-----+---------------+-----------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crime_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_df.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode(\"append\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
