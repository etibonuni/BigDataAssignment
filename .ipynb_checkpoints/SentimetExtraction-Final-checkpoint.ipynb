{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up HDFS and Google credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://sp-master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://10.164.0.2:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Test Etienne JOB</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://10.164.0.2:7077 appName=Test Etienne JOB>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "LOCAL_IP = \"10.164.0.2\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Test Etienne JOB\") \\\n",
    "    .master(\"spark://10.164.0.2:7077\") \\\n",
    "    .config(\"spark.executor.cores\", 2) \\\n",
    "    .config(\"spark.cores.max\", 18) \\\n",
    "    .config(\"spark.python.worker.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.memory\", \"5g\") \\\n",
    "    .config(\"spark.executorEnv.SPARK_LOCAL_IP\", LOCAL_IP) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.140805\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from random import random\n",
    "from operator import add\n",
    "\n",
    "\n",
    "partitions = 200\n",
    "n = 1000000 * partitions\n",
    "\n",
    "def f(_):\n",
    "    x = random() * 2 - 1\n",
    "    y = random() * 2 - 1\n",
    "    return 1 if x ** 2 + y ** 2 <= 1 else 0\n",
    "\n",
    "count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"./imdb-e9e7ce7a779d.json\"\n",
    "os.environ[\"HDFSCLI_CONFIG\"]=\"./.hdfscli.cfg\"\n",
    "os.environ[\"HADOOP_CONF_DIR\"]=\"/opt/hadoop-3.1.0/etc/hadoop\"\n",
    "sc.environment[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/MovieScope-1bf4856cc738.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List filenames of reviews from HDFS and parallelize in preparation from processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelise the reviews and use Google NLP API to extract entities and related sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the Google Cloud client library\n",
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "from functools import reduce\n",
    "\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "from pyspark.sql import functions\n",
    "import re\n",
    "import time\n",
    "\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectEntities(x, y):\n",
    "    # The first reduce call doesn't pass a list for x, so we need to check for that.\n",
    "    if not isinstance(x, list):\n",
    "        x=[x]\n",
    "        \n",
    "\n",
    "    xd = dict(x)\n",
    "    #print(xd)\n",
    "    \n",
    "    if not isinstance(y, list):\n",
    "        y = [y]\n",
    "        \n",
    "    for ye in y:\n",
    "        if ye[0] in xd:\n",
    "            try:\n",
    "                xd[ye[0]] = (xd[ye[0]]+ye[1])/2\n",
    "            except:\n",
    "                Null\n",
    "        else:\n",
    "            xd[ye[0]] = ye[1]\n",
    "    \n",
    "    return [o for o in xd.items()]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "orientation = \"pos\"\n",
    "collection=\"reviews\"\n",
    "urlsCollection=\"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = sc.wholeTextFiles(\"hdfs://sp-master:8020/user/lmrd/\"+collection+\"/\"+orientation)\n",
    "tf = tf.repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def checkSentimentValue(x):\n",
    "    try:\n",
    "        f = float(x)\n",
    "        \n",
    "        return f\n",
    "    \n",
    "    except:\n",
    "        print(\"Wrong sentiment value \", f)\n",
    "        return 0\n",
    "    \n",
    "def extractEntitiesSetiment2(fileObj):\n",
    "    # Instantiates a client\n",
    "    client = language.LanguageServiceClient()\n",
    "    \n",
    "    review_contents = fileObj[1]\n",
    "        \n",
    "    #print(review_contents)\n",
    "    document = types.Document(content = review_contents, \n",
    "                             type=enums.Document.Type.PLAIN_TEXT, language=\"en-US\")\n",
    "    \n",
    "    tries=1\n",
    "    \n",
    "    while tries < 5:\n",
    "        try:\n",
    "            entities = client.analyze_entity_sentiment(document=document, encoding_type=\"UTF8\")\n",
    "            break\n",
    "        except:\n",
    "            f = open(\"/home/etienne/sparklog.txt\", mode=\"a\")\n",
    "            f.write(\"\"+str(fileObj[0])+\"\\n\")\n",
    "            f.write(\"\"+str(entities)+\"\\n\")\n",
    "            f.close()\n",
    "            time.sleep(1)\n",
    "            \n",
    "            tries +=1\n",
    "    \n",
    "    # Make sure we have no duplicate entities. If we do, average their sentiment.\n",
    "    justLetters = re.compile(\"[^a-z ]\")\n",
    "    response = [o for o in zip([lemmatizer(justLetters.sub(\"\", entity.name.lower()), u\"NOUN\")[0] for entity in entities.entities], \n",
    "                               [checkSentimentValue(entity.sentiment.score) * checkSentimentValue(entity.sentiment.magnitude) \n",
    "                                    for entity in entities.entities])]\n",
    "    \n",
    "#    response = sorted(response, key=lambda x: x[0])\n",
    "#    if (len(response)>1):\n",
    "#        response = reduce(collectEntities, response)\n",
    "    \n",
    "        \n",
    "    #print(fileObj[0], response)\n",
    "    try:\n",
    "        fid = int(fileObj[0])\n",
    "    except:\n",
    "        fid=0\n",
    "    \n",
    "    return (fid, response)\n",
    "\n",
    "def extractOrdering(rec):\n",
    "    filenameRegexp = \".*/([0-9]*)_.*\\.txt$\"\n",
    "    r = re.search(filenameRegexp, rec[0])\n",
    "\n",
    "    return (int(r.groups()[0])+1, rec[1])\n",
    "    #hdfs://localhost:9000/user/lmrd/reviews/pos/3467_7.txt\n",
    "\n",
    "\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "sc.broadcast(lemmatizer)\n",
    "\n",
    "filesRdd = tf.map(extractOrdering)\n",
    "filesRdd = filesRdd.repartition(5)\n",
    "\n",
    "schema1 = StructType([\n",
    "    StructField(\"ID\", IntegerType(), False),\n",
    "    StructField(\"ENTITY_SENTIMENT\", ArrayType(\n",
    "            StructType([StructField(\"ENTITY\", StringType(), False), \n",
    "                        StructField(\"SENTIMENT\", FloatType(), False)])), nullable=True)])\n",
    "\n",
    "\n",
    "entity_documents_info = filesRdd.map(extractEntitiesSetiment2)\n",
    "\n",
    "entity_documents_info.cache()\n",
    "#entity_documents_info.saveAsTextFile(\"hdfs://sp-master:8020/user/lmrd/reviews/temp_pos3.txt\")\n",
    "\n",
    "\n",
    "entity_documents_info = spark.createDataFrame(filesRdd.map(extractEntitiesSetiment2), schema1)#schema=[\"ID\", \"ENTITIY_SENTIMENT\"])\n",
    "\n",
    "#entity_documents_info = entity_documents_info.rdd.repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#entity_documents_info = spark.createDataFrame(filesRdd.map(extractEntitiesSetiment2), schema=[\"ID\", \"ENTITIY_SENTIMENT\"])\n",
    "\n",
    "entity_documents_info.write.parquet(\"hdfs://spark-master:8020/user/lmrd/\"+collection+\"/\"+orientation+\"_doc_info2.pq\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
